---
sidebar_position: 2
---

# Hugging Face

Properties used to connect to [Hugging Face API](https://learn.microsoft.com/en-gb/azure/cognitive-services/).

### `huggingFace` {#huggingFace}

- Type: { <br />
  &nbsp;&nbsp;&nbsp;&nbsp; [`conversation?: Conversation`](HERE), <br />
  &nbsp;&nbsp;&nbsp;&nbsp; [`textGeneration?: TextGeneration`](HERE), <br />
  &nbsp;&nbsp;&nbsp;&nbsp; [`summarization?: Summarization`](HERE), <br />
  &nbsp;&nbsp;&nbsp;&nbsp; [`translation?: Translation`](HERE), <br />
  &nbsp;&nbsp;&nbsp;&nbsp; [`fillMask?: FillMask`](HERE), <br />
  &nbsp;&nbsp;&nbsp;&nbsp; [`questionAnswer?: QuestionAnswer`](HERE), <br />
  &nbsp;&nbsp;&nbsp;&nbsp; [`audioSpeechRecognition?: AudioSpeechRecognition`](HERE), <br />
  &nbsp;&nbsp;&nbsp;&nbsp; [`audioClassification?: AudioClassification`](HERE), <br />
  &nbsp;&nbsp;&nbsp;&nbsp; [`imageClassification?: ImageClassification`](HERE) <br />
  }

- Default: _{conversation: true}_

import ContainersKeyToggle from '@site/src/components/table/containersKeyToggle';
import ComponentContainer from '@site/src/components/table/componentContainer';
import DeepChatBrowser from '@site/src/components/table/deepChatBrowser';
import LineBreak from '@site/src/components/markdown/lineBreak';
import BrowserOnly from '@docusaurus/BrowserOnly';
import TabItem from '@theme/TabItem';
import Tabs from '@theme/Tabs';

<BrowserOnly>{() => require('@site/src/components/nav/autoNavToggle').readdAutoNavShadowToggle()}</BrowserOnly>

## Service Types

### `Conversation` {#Conversation}

- Type: `true` | {<br />
  &nbsp;&nbsp;&nbsp;&nbsp; `model?: string`, <br />
  &nbsp;&nbsp;&nbsp;&nbsp; `parameters?:` { <br />
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; `min_length?: string`, <br />
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; `max_length?: string`, <br />
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; `top_k?: string`, <br />
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; `top_p?: string`, <br />
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; `temperature?: string`, <br />
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; `repetition_penalty?: string`}, <br />
  &nbsp;&nbsp;&nbsp;&nbsp; `options?:` {`use_cache?: boolean`} <br />
  }

- Default: _{model: "facebook/blenderbot-400M-distill", options: {use_cache: true}}_

Connect to Hugging Face [`Conversational`](https://huggingface.co/docs/api-inference/detailed_parameters#conversational-task) API. <br />
`model` is the name of the model used for the task. <br />
`min_length` is the minimum length in tokens of the output summary. <br />
`max_length` is the maximum length in tokens of the output summary. <br />
`top_k` defines the top tokens considered within the sample operation to create new text. <br />
`top_p` is a float to define the tokens that are within the sample operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than top \* p. <br />
`temperature` is a float (ranging from _0.0_ to _100.0_) temperature of the sampling operation. 1 means regular sampling, _0_ means always take the highest score, _100.0_ is getting closer to uniform probability. <br />
`repetition_penalty` is a float (ranging from _0.0_ to _100.0_) that controls where a token is used more within generation the more it is penalized to not be picked in successive generation passes. <br />
`use_cache` is used to speed up requests by using the inference API cache.

#### Example

<ContainersKeyToggle>
  <ComponentContainer>
    <DeepChatBrowser
      existingService={{
        huggingFace: {
          key: 'placeholder key',
          conversation: {model: 'facebook/blenderbot-400M-distill', parameters: {temperature: 1}},
        },
      }}
    ></DeepChatBrowser>
  </ComponentContainer>
  <ComponentContainer>
    <DeepChatBrowser
      existingService={{
        huggingFace: {
          conversation: {model: 'facebook/blenderbot-400M-distill', parameters: {temperature: 1}},
        },
      }}
    ></DeepChatBrowser>
  </ComponentContainer>
</ContainersKeyToggle>

<LineBreak></LineBreak>

### `TextGeneration` {#TextGeneration}

- Type: `true` | {<br />
  &nbsp;&nbsp;&nbsp;&nbsp; `model?: string`, <br />
  &nbsp;&nbsp;&nbsp;&nbsp; `parameters?:` { <br />
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; `top_k?: string`, <br />
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; `top_p?: string`, <br />
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; `temperature?: string`, <br />
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; `repetition_penalty?: string`, <br />
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; `max_new_tokens?: string`, <br />
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; `do_sample?: boolean`}, <br />
  &nbsp;&nbsp;&nbsp;&nbsp; `options?:` {`use_cache?: boolean`} <br />
  }

- Default: _{model: "gpt2", options: {use_cache: true}}_

Connect to Hugging Face [`Text Generation`](https://huggingface.co/docs/api-inference/detailed_parameters#text-generation-task) API. <br />
`model` is the name of the model used for the task. <br />
`top_k` defines the top tokens considered within the sample operation to create new text. <br />
`top_p` is a float to define the tokens that are within the sample operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than top \* p. <br />
`temperature` is a float (ranging from _0.0_ to _100.0_) temperature of the sampling operation. 1 means regular sampling, _0_ means always take the highest score, _100.0_ is getting closer to uniform probability. <br />
`repetition_penalty` is a float (ranging from _0.0_ to _100.0_) that controls where a token is used more within generation the more it is penalized to not be picked in successive generation passes. <br />
`max_new_tokens` is an integer (ranging from _0_ to _250_) amount of new tokens to be generated by the response. <br />
`do_sample` controls whether or not to use sampling. If `false` it uses greedy decoding sampling. <br />
`use_cache` is used to speed up requests by using the inference API cache.

#### Example

<ContainersKeyToggle>
  <ComponentContainer>
    <DeepChatBrowser
      existingService={{
        huggingFace: {
          key: 'placeholder key',
          textGeneration: {model: 'gpt2', parameters: {temperature: 1}},
        },
      }}
    ></DeepChatBrowser>
  </ComponentContainer>
  <ComponentContainer>
    <DeepChatBrowser
      existingService={{
        huggingFace: {
          textGeneration: {model: 'gpt2', parameters: {temperature: 1}},
        },
      }}
    ></DeepChatBrowser>
  </ComponentContainer>
</ContainersKeyToggle>

<LineBreak></LineBreak>

### `Summarization` {#Summarization}

- Type: `true` | {<br />
  &nbsp;&nbsp;&nbsp;&nbsp; `model?: string`, <br />
  &nbsp;&nbsp;&nbsp;&nbsp; `parameters?:` { <br />
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; `min_length?: string`, <br />
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; `max_length?: string`, <br />
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; `top_k?: string`, <br />
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; `top_p?: string`, <br />
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; `temperature?: string`, <br />
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; `repetition_penalty?: string`}, <br />
  &nbsp;&nbsp;&nbsp;&nbsp; `options?:` {`use_cache?: boolean`} <br />
  }

- Default: _{model: "facebook/bart-large-cnn", options: {use_cache: true}}_

Connect to Hugging Face [`Summarization`](https://huggingface.co/docs/api-inference/detailed_parameters#summarization-task) API. <br />
`model` is the name of the model used for the task. <br />
`min_length` is the minimum length in tokens of the output summary. <br />
`max_length` is the maximum length in tokens of the output summary. <br />
`top_k` defines the top tokens considered within the sample operation to create new text. <br />
`top_p` is a float to define the tokens that are within the sample operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than top \* p. <br />
`temperature` is a float (ranging from _0.0_ to _100.0_) temperature of the sampling operation. 1 means regular sampling, _0_ means always take the highest score, _100.0_ is getting closer to uniform probability. <br />
`repetition_penalty` is a float (ranging from _0.0_ to _100.0_) that controls where a token is used more within generation the more it is penalized to not be picked in successive generation passes. <br />
`use_cache` is used to speed up requests by using the inference API cache.

#### Example

<ContainersKeyToggle>
  <ComponentContainer>
    <DeepChatBrowser
      existingService={{
        huggingFace: {
          key: 'placeholder key',
          summarization: {model: 'facebook/bart-large-cnn', parameters: {temperature: 1}},
        },
      }}
    ></DeepChatBrowser>
  </ComponentContainer>
  <ComponentContainer>
    <DeepChatBrowser
      existingService={{
        huggingFace: {
          summarization: {model: 'facebook/bart-large-cnn', parameters: {temperature: 1}},
        },
      }}
    ></DeepChatBrowser>
  </ComponentContainer>
</ContainersKeyToggle>

<LineBreak></LineBreak>

### `Translation` {#Translation}

// WORK - does not throw an error

- Type: `true` | {<br />
  &nbsp;&nbsp;&nbsp;&nbsp; `model?: string`, <br />
  &nbsp;&nbsp;&nbsp;&nbsp; `options?:` {`use_cache?: boolean`} <br />
  }

- Default: _{model: "Helsinki-NLP/opus-tatoeba-en-ja", options: {use_cache: true}}_

Connect to Hugging Face [`Translation`](https://huggingface.co/docs/api-inference/detailed_parameters#translation-task) API. <br />
`model` is the name of the model used for the task. <br />
`use_cache` is used to speed up requests by using the inference API cache.

#### Example

<ContainersKeyToggle>
  <ComponentContainer>
    <DeepChatBrowser
      existingService={{
        huggingFace: {
          key: 'placeholder key',
          translation: {model: 'Helsinki-NLP/opus-tatoeba-en-ja'},
        },
      }}
    ></DeepChatBrowser>
  </ComponentContainer>
  <ComponentContainer>
    <DeepChatBrowser
      existingService={{
        huggingFace: {
          translation: {model: 'Helsinki-NLP/opus-tatoeba-en-ja'},
        },
      }}
    ></DeepChatBrowser>
  </ComponentContainer>
</ContainersKeyToggle>

<LineBreak></LineBreak>

### `FillMask` {#FillMask}

- Type: `true` | {<br />
  &nbsp;&nbsp;&nbsp;&nbsp; `model?: string`, <br />
  &nbsp;&nbsp;&nbsp;&nbsp; `options?:` {`use_cache?: boolean`} <br />
  }

- Default: _{model: "bert-base-uncased", options: {use_cache: true}}_

Connect to Hugging Face [`Fill Mask`](https://huggingface.co/docs/api-inference/detailed_parameters#fill-mask-task) API. <br />
`model` is the name of the model used for the task. <br />
`use_cache` is used to speed up requests by using the inference API cache.

#### Example

<ContainersKeyToggle>
  <ComponentContainer>
    <DeepChatBrowser
      existingService={{
        huggingFace: {
          key: 'placeholder key',
          fillMask: {model: 'bert-base-uncased'},
        },
      }}
    ></DeepChatBrowser>
  </ComponentContainer>
  <ComponentContainer>
    <DeepChatBrowser
      existingService={{
        huggingFace: {
          fillMask: {model: 'bert-base-uncased'},
        },
      }}
    ></DeepChatBrowser>
  </ComponentContainer>
</ContainersKeyToggle>

<LineBreak></LineBreak>

### `QuestionAnswer` {#QuestionAnswer}

- Type: `true` | {`context: string`, model?: string`}
- Default: _{model: "bert-large-uncased-whole-word-masking-finetuned-squad"}_

Connect to Hugging Face [`Question Answer`](https://huggingface.co/docs/api-inference/detailed_parameters#question-answering-task) API. <br />
`context` is a string containing details that AI can use to answer the given questions. <br />
`model` is the name of the model used for the task. <br />

#### Example (Ask about labrador looks)

<ContainersKeyToggle>
  <ComponentContainer>
    <DeepChatBrowser
      existingService={{
        huggingFace: {
          key: 'placeholder key',
          questionAnswer: {
            context:
              'Labrador retrievers are easily recognized by their broad head, drop ears and large, expressive eyes. Two trademarks of the Lab are the thick but fairly short double coat, which is very water repellent, and the well known otter tail. The tail is thick and sturdy and comes off the topline almost straight.',
          },
        },
      }}
    ></DeepChatBrowser>
  </ComponentContainer>
  <ComponentContainer>
    <DeepChatBrowser
      existingService={{
        huggingFace: {
          questionAnswer: {
            context:
              'Labrador retrievers are easily recognized by their broad head, drop ears and large, expressive eyes. Two trademarks of the Lab are the thick but fairly short double coat, which is very water repellent, and the well known otter tail. The tail is thick and sturdy and comes off the topline almost straight.',
          },
        },
      }}
    ></DeepChatBrowser>
  </ComponentContainer>
</ContainersKeyToggle>

<LineBreak></LineBreak>

### `AudioSpeechRecognition` {#AudioSpeechRecognition}

- Type: `true` | {model?: string`}
- Default: _{model: "facebook/wav2vec2-large-960h-lv60-self"}_

Connect to Hugging Face [`Audio Speech Recognition`](https://huggingface.co/docs/api-inference/detailed_parameters#automatic-speech-recognition-task) API. <br />
`model` is the name of the model used for the task. <br />

#### Example

<ContainersKeyToggle>
  <ComponentContainer>
    <DeepChatBrowser
      existingService={{
        huggingFace: {
          key: 'placeholder key',
          audioSpeechRecognition: {model: 'facebook/wav2vec2-large-960h-lv60-self'},
        },
      }}
    ></DeepChatBrowser>
  </ComponentContainer>
  <ComponentContainer>
    <DeepChatBrowser
      existingService={{
        huggingFace: {
          audioSpeechRecognition: {model: 'facebook/wav2vec2-large-960h-lv60-self'},
        },
      }}
    ></DeepChatBrowser>
  </ComponentContainer>
</ContainersKeyToggle>

<LineBreak></LineBreak>

### `AudioClassification` {#AudioClassification}

- Type: `true` | {model?: string`}
- Default: _{model: "ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition"}_

Connect to Hugging Face [`Audio Classification`](https://huggingface.co/docs/api-inference/detailed_parameters#audio-classification-task) API. <br />
`model` is the name of the model used for the task. <br />

#### Example

<ContainersKeyToggle>
  <ComponentContainer>
    <DeepChatBrowser
      existingService={{
        huggingFace: {
          key: 'placeholder key',
          audioClassification: {model: 'ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition'},
        },
      }}
    ></DeepChatBrowser>
  </ComponentContainer>
  <ComponentContainer>
    <DeepChatBrowser
      existingService={{
        huggingFace: {
          audioSpeechRecognition: {model: 'ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition'},
        },
      }}
    ></DeepChatBrowser>
  </ComponentContainer>
</ContainersKeyToggle>

<LineBreak></LineBreak>

### `ImageClassification` {#ImageClassification}

- Type: `true` | {model?: string`}
- Default: _{model: "google/vit-base-patch16-224"}_

Connect to Hugging Face [`Image Classification`](https://huggingface.co/docs/api-inference/detailed_parameters#image-classification-task) API. <br />
`model` is the name of the model used for the task. <br />

#### Example

<ContainersKeyToggle>
  <ComponentContainer>
    <DeepChatBrowser
      existingService={{
        huggingFace: {
          key: 'placeholder key',
          imageClassification: {model: 'google/vit-base-patch16-224'},
        },
      }}
    ></DeepChatBrowser>
  </ComponentContainer>
  <ComponentContainer>
    <DeepChatBrowser
      existingService={{
        huggingFace: {
          imageClassification: {model: 'google/vit-base-patch16-224'},
        },
      }}
    ></DeepChatBrowser>
  </ComponentContainer>
</ContainersKeyToggle>

<LineBreak></LineBreak>
